OPENMPI:

install-sudo apt-get install libopenmpi-dev;  sudo apt-get install openmpi-bin

PROGRAM:
#include <stdio.h>
#include <omp.h>
int main()
{
int sum=0;
int i;
#pragma omp parallel
{
int localsum=0;
#pragma omp for
for(i=0;i<10;i++)
{
localsum+=i;
}
#pragma omp critical
sum+=localsum;
}}
printf("%d",sum);
return 0;
}

COMPILE: gcc -fopenmp hello_world.c -o hello_world.out
RUN: ./hello_world.out

MPICH:

program:
#include<stdio.h>
#include<mpi.h>
int main(int argv,char ** argc){
int rank,size;
{
MPI_Init(NULL,NULL);
MPI_Comm_rank(MPI_COMM_WORLD,&rank);
MPI_Comm_size(MPI_COMM_WORLD,&size);
MPI_Finalize();
printf("HELLO WORLD FROM %d OUT OF TOTAL PROCESDS %d\n",rank,size);
}
printf("program ends");
return 0;
}

COMPILE: mpicc hello.c -o hello
RUN:mpirun -np -4 ./hello

SPARK:
sudo apt update
java -version
sudo apt-get install default-jdk
sudo apt-get install scala
scala -version
Download Apache Spark from google
cd Downloads
ls (spark-3.5.3-bin-hadoop3.tgz)
tar xvf spark-3.5.3-bin-hadoop3.tgz
sudo su -
cd /home/tce/Downloads/
mv spark-3.5.3-bin-hadoop3 /usr/local/sparkfile
cd /usr/local
ls
cd bin
source ~/.bashrc
export PATH=$PATH:/usr/local/sparkfile/bin
spark-shell
open a new terminal,sudo su -
file create in /usr/local/bin(input.txt)
back to spark-shell terminal,
val inputfile=sc.textFile("input.txt")
val counts=inputfile.flatMap(line=> line.split(" ")).map(word=>(word,1)).reduceByKey(_+_)
counts.saveAsTextFile("output")
exit or ctrl + C
cd /usr/local/bin/output
ls (part-00000)
cat part-00000





DOCKER:
docker --version
sudo usermod -aG docker $USER -->Adds the current user ($USER) to the Docker group (docker).
sudo systemctl start docker -->Starts the Docker service.
sudo systemctl status docker --> Shows the current status of the Docker service
mkdir test
cd test
touch readme.txt
touch DockerFile
ls
gedit DockerFile
FROM ubuntu
CMD ["echo","hello i am docker"]S
sudo docker build -t mydocker .
sudo docker images
sudo docker run mydocker
sudo chown -R $(whoami) ~/.docker
docker login
docker images
docker ps
docker ps -a
docker pull ubuntu
docker run ubuntu
docker run -d ubuntu
docker run -it ubuntu /bin/bash

CloudSIM and cloudAnalyst

CloudSIM:
Download .zip file from the links
https://github.com/Cloudslab/cloudsim/releases           // (5.0)
https://commons.apache.org/math/download_math.cgi
(filename: commons-math4-4.0-beta.1-bin) -->extract it
Download Eclipse(x86_64)
Open eclipse,file ->new -> java pro->proj name --> cloudsim
disable default location and provide the path of cloudsim 5.0 download loc
click finish

Right click the cloudsim->properties->java build path->libraries->click module path->add external JAR
 in that (extract the math download)
Click apply
Go back to eclipse,open cloudsim(expand it)-->open cloudsim-examples-->org.cloudbus-->cloud example-->double click it(code will be visible) and run it 

cloudAnalyst:

https://sourceforge.net/projects/cloudanalystnetbeans/
https://netbeans.apache.org/front/main/download/nb17/     // (under installers and packages,first link)
Open project->cloud analyst,run (error comes also give ok)
configure simulations and run simulations


Hadoop

Check for Java   -> java -version
download java then,
Go to Program files,check for java and cut the jdk and put it into another folder "java" outside the program files.
//coresite.xml
<property>
<name>fs.defaultFS</name>
<value>hdfs://localhost:9000</value>
</property>
//httpfs-site.xml
<property>
<name>dfs.replication</name>
<value>1</value>
</property>

in Hadoop create a folder called "data" --> namenode and datanode folder (copy the path until datanode)

<property>
<name>dfs.datanode.data.dir</name>
<value>path of the datanode</value>
</property>
<property>
<name>dfs.namenode.name.dir</name>
<value>path of the namenode</value>
</property>

mapred site .xml
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>

yarn-site.xml
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
<property>
<name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>

go to Hadoop and del the bin folder.

https://drive.google.com/file/d/1nCN_jK7EJF2DmPUUxgOggnvJ6k6tksYz/view?usp=sharing
msvcr 120 dll(put this into c:windows:system32:)
msvc-170
hdfs namenode -format
goto sbin folder cmd ,
start-dfs.cmd
jps
start-yarn.cmd
 goto chrome=>localhost:9870,localhost:8088